---
title: "Prediction of Exercise Execution"
author: "Teresa Wilson"
date: "May 14, 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r message = FALSE, echo = FALSE}
# Set global options and load libraries
knitr::opts_chunk$set(echo = TRUE, comment = NA)
library(caret)
```

## Synopsis

A random forest model was created to predict how well 6 subjects performed a weightlifting exercise based on data collected by on-body sensors. The model was able to predict the type of exercise "mistake" with an accuracy of 0.96. The model was then used to predict a set of 20 test cases.

## Introduction

The quantified-self movement generates a large amount of data from devices such as *Jawbone Up, Nike FuelBand,* and *Fitbit*. Human activiy recognition has focused on quantifying how *much* of an activity people do, but it has rarely been used to quantify how *well* they do the activity.

Six participants performed repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D), and throwing the hips to the front (Class E). Class A was the "correct" way to perform the exercise, and Classes B-E were "mistakes".

Motion was measured by sensors placed on the glove, armband, belt, and dumbbell. The sensors consisted of an accelerometer, a gyroscope, and a magentometer, generating a total of 153 different variables for each observation.

More information on the data can be found at http://groupware.les.inf.puc-rio.br/har and in reference [1] at the end of this report.

This study attempts to predict the manner in which the participant performed the exercise.

## Read Data

First, we'll read in the data. Two `csv` files are provided, a training file and a testing file. 

```{r cache = TRUE}
training <- read.csv("pml-training.csv", sep = ",", header = TRUE)
testCaseData <- read.csv("pml-testing.csv", sep = ",", header = TRUE)
```

The data from the `pml-testing.csv` file is actually a set of 20 test cases to be evaluated with the final model, and the data variable is named accordingly.

## Exploratory Analysis

```{r echo = FALSE, eval = FALSE}
head(training)
tail(training)
summary(training)
names(training)
summary(training[which(training$classe == "A"), ])
str(training)
```

The dimensions of the training set are 

```{r}
dim(training)
```

Because the dimensions are so large, we'll only display the first few rows and a few columns at a time. 

The first 7 columns are data about the subject and measurement conditions. These columns will not be included as input variables to the model because they are not data generated by the sensors.

```{r}
training[1:5, 1:7]
```

Now we'll look at the last few columns:

```{r}
training[1:5, (ncol(training)-5):ncol(training)]
```

The last column is the variable `classe`, which will be the response variable we'll train the model on.

As shown in the bar plot, the outcome variable is approximately evenly distributed across all values, but with a larger number of the class `A`.

```{r}
barplot(table(training$classe), main = "Frequency of Outcome Classes",
        ylim = c(0, 7000), xlab = "Exercise Label",
        ylab = "Count")
```

```{r echo = FALSE, eval = FALSE}
head(testCaseData)
tail(testCaseData)
summary(testCaseData)
names(testCaseData)
```

The input data for the test cases have only 20 observations but the same number of variables:

```{r}
dim(testCaseData)
```

We can look at all the rows and the last few columns to get an idea of the data:

```{r}
testCaseData[ , (ncol(testCaseData)-4):ncol(testCaseData)]
```

For the test case data, the last column is the problem ID (or test case ID) rather than the response `classe`.

## Data Preprocessing

The data were preprocessed to create a training set that could be input to the model. The steps included creating a subset of observations for the training set, removing variables that could not be used as features, and removing missing values.

### Create training data set

Due to memory limitations of the computer used for the calculations, the size of the training set was constrained. The computer could train the model on a data set up to a maximum size of 25% of the original data set before running into memory problems. So a training set equal to 25% of the original data set was created using the `createDataPartition` function from the `caret` package, which randomly samples the data set while maintaining the original frequencies of the response values (the variable `classe`). 

Note: The random forest algorithm used in the model calculates out-of-bag error in the course of the calculations, and this can be used as an estimate for the out-of-sample error. That would mean there is no need for a testing data set, but we'll set aside the remaining 75% of the data anyway as a testing set to use as an additional check. 

The seed is set beforehand to create reproducible results. 

```{r}
set.seed(1000)
inTrain = createDataPartition(training$classe, p = .25)[[1]]
trainingSplit = training[ inTrain,]
testingSplit = training[-inTrain,]
```

This gives a training set with `r dim(trainingSplit)[1]` observations and `r dim(trainingSplit)[2]` variables.

### Remove unnecessary columns 

The first 7 columns are removed from the data sets. These are information about the subject and the measurement conditions and cannot be used as features in the model.

```{r}
trainData <- trainingSplit[ , 8:ncol(trainingSplit)]
testData <- testingSplit[ , 8:ncol(testingSplit)]  
testCaseData <- testCaseData[ , 8:ncol(testCaseData)]
```

### Remove missing values

Several of the variables contain no data (i.e., they have value `NA` for all observations). These variables need to be removed from the data set because you can't model data that isn't there. We'll determine the variables in the training set that are all `NA`s.

```{r}
indexNA_train <- colSums(is.na(trainData)) != 0
sum(indexNA_train)  
```

In the training data set, `r sum(indexNA_train)` of the variables are all `NA`s.

We do the same for the test case data set.

```{r}
indexNA_testCase <- colSums(is.na(testCaseData)) != 0
sum(indexNA_testCase)  
```

In the test case data set, `r sum(indexNA_testCase)` of the variables are all `NA`s. Since there are a total of `r dim(testCaseData)[2]` variables, this means that there are `r dim(testCaseData)[2] - sum(indexNA_testCase)` variables that are not all `NA`s. This is the maximum subset that we should train the model on because if the entire column is missing you can't do imputation.

We subset the training data to remove variables found in the test case data set that are all `NA`s.

```{r}
trainData <- subset(trainData, select = !indexNA_testCase)
dim(trainData) 
```

We check to make sure there are no missing values in the training data set.

```{r}
sum(is.na(trainData))
```

This is now the data set that we'll use to train the model. It contains `r dim(trainData)[2]` columns: `r dim(trainData)[2]-1` features and one column (`classe`) that is the response variable.

## Description of Model

The random forest algorithm was chosen for the model because this is a classification problem and we expect the data to have a weak signal and a high level of noise noise due to the exercise movements that were performed.

### Cross-validation

Ten-fold cross-validation was used in the random forest model to determine the variable importance and the variables used at each tree node.

### Out-of-sample error estimate

The random forest method (`rf`) in the `caret` package automatically calculates an "out-of-bag" (OOB) error estimate when fitting trees to resampled subsets of the data, so this value can be used as an estimate for the out-of-sample error.

## Train model

The model was trained on the training data set using all `r dim(trainData)[2]-1` features. The maximum number of trees was set at 10 due to memory limitations of the computer.
 
```{r message = FALSE}
set.seed(2000)
modFit <- train(classe ~ ., 
                method = "rf", 
                data = trainData, 
                prox = TRUE, 
                ntree = 10,
                trControl = trainControl(method = "cv"))
```

## Results

### Out-of-sample error estimate

The out-of-bag error estimate was 7.63%, which is also an estimate for the out-of-sample error. The confusion matrix is shown with the error estimates for each class.

```{r}
modFit$finalModel
```

### Accuracy 

The accuracy for the final model was 0.96.

```{r}
modFit
```

This plot shows the accuracy of the cross-validation as a function of the number of randomly selected predictors. The model reaches maximum accuracy using about half of the predictors.

```{r}
plot(modFit, main = "Accuracy vs. Number of Predictors",
     xlab = "Number of Predictors")  
```

### Variable Importance

The variable importance determines the most important variables in the model,  where the importance is scaled from 0 to 100.

```{r message = FALSE}
varImp(modFit)
```

The next plot shows the same information graphically. The top 7 variables have the largest relative importance, and the remaining variables have much lower importance.

```{r}
plot(varImp(modFit), top=20, main = "Variable Importance")
```

### Feature selection 

The random forests method determines the important features as it calculates the model on all the features available. Another way to determine the important features is to do feature selection before running the model by removing the features that are highly correlated. The model is then run with this subset of features, which reduces computation time. We will try this method of feature selection to see if it gives better accuracy.

First, the correlation matrix of all features is calculated.

```{r}
set.seed(2)
correlationMatrix <- cor(trainData[ , 1:(ncol(trainData)-1)])
```

If the correlation is greater than 0.25, then those features are removed from the data set because they are highly correlated. 

```{r}
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.25)
trainDataSub <- trainData[ , -highlyCorrelated]
```

The resulting data set contains only the following features.

```{r}
names(trainDataSub)
```

This creates a data set with `r dim(trainDataSub)[2]-1` features that are used to train the model with the same parameters as for the model with the full feature set. The results of this model are below.

```{r echo = FALSE, message = FALSE}
set.seed(2000)
modFitSub <- train(classe ~ ., 
                method = "rf", 
                data = trainDataSub, 
                prox = TRUE, 
                ntree = 10,
                trControl = trainControl(method = "cv"))

modFitSub
```

With the reduced number of features, the accuracy is only 0.90, which is less than the accuracy of 0.96 obtained with the full feature set. 

We can plot the variable importance again and see that the model ranked the variables in a different order this time. Selecting variables via the correlation matrix can omit influential variables, which affects the accuracy and the importance of individual variables.

```{r}
plot(varImp(modFitSub), top = (dim(trainDataSub)[2] - 1),
     main = "Variable Importance (Selected by Correlation)")
```


## Prediction Using the Model

The random forest algorithm calculates the out-of-sample error (really the out-of-bag error) automatically during the course of the optimization, but it is interesting to calculate it on a separate test set anyway to compare the accuracy.

### Prediction on test set 

The test data set, with `r dim(testData)[1]` observations, was used to predict the class values.

```{r}
dim(testData)[1] 
```

The model using the full number of features was used to predict on the test data.

```{r}
predTest <- predict(modFit, newdata = testData)
confusionMatrix(data = predTest, testData$classe)
```

The accuracy was 0.9585, which is close to the OOB error of 0.96 calculated from the model. Therefore, the OOB error calculated on the training set was a good estimate of the out-of-sample error.

### Prediction on test cases 

Next, the model using the full number of features was used to predict on the 20 test cases.

```{r, results='asis'}
predTestCase <- predict(modFit, newdata = testCaseData)
truthClass <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A", "B", 
                "C", "B", "A", "E", "E", "A", "B", "B", "B")
knitr::kable(data.frame(testCase = as.factor(testCaseData$problem_id), 
                        predClass = predTestCase, truthClass))
```

The model was able to correctly predict all 20 test cases.

## Conclusion

A random forest model was able to predict with 0.96 accuracy how well a participant performed an exercise. Using a computer with more memory may increase the accuracy because a larger training set and a larger number of trees could be used. 

## References

[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013.


